{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "import my_models as models\n",
    "import my_utils as ut\n",
    "import attacks as at\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from time import ctime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    data = 'cifar10'\n",
    "    num_epochs = 100\n",
    "    bs = 256\n",
    "    data_augment=False\n",
    "    early_stop=False\n",
    "    relax_alpha = 0\n",
    "    num_workers = 4\n",
    "    num_classes=10\n",
    "    device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets & dataloaders\n",
    "tr_ds, te_ds = ut.get_datasets(args.data, augment=args.data_augment)\n",
    "v_size = int(len(tr_ds)*0.1)\n",
    "tr_ds, v_ds = torch.utils.data.random_split(tr_ds, [len(tr_ds)-v_size, v_size])\n",
    "tr_loader = DataLoader(tr_ds, batch_size=args.bs, shuffle=True, num_workers=args.num_workers, pin_memory=True, persistent_workers=True)\n",
    "v_loader = DataLoader(v_ds, batch_size=args.bs, shuffle=False, num_workers=args.num_workers, pin_memory=True, persistent_workers=True)\n",
    "te_loader = DataLoader(te_ds, batch_size=args.bs, shuffle=False, num_workers=args.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a98545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model & loss & optimizer & lr scheduler\n",
    "model = models.resnet20().to(args.device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#opt = optim.Adam(model.parameters())\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=5, verbose=True)\n",
    "opt = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=[35, 75, 90], gamma=0.1, verbose=False)\n",
    "es = ut.EarlyStopping(patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bde6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loggers\n",
    "file_name = f\"\"\"time:{ctime()}_data:{args.data}_epoch:{args.num_epochs}_earlyStop:{args.early_stop}\"\"\"\\\n",
    "            +f\"\"\"_dataAugment:{args.data_augment}_relaxAlpha:{args.relax_alpha}\"\"\"\n",
    "writer = SummaryWriter('../logs/' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439512ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in tqdm(range(args.num_epochs)):\n",
    "    model.train()\n",
    "    tr_loss, tr_acc = 0, 0\n",
    "    for _, (inp, lbl) in enumerate(tr_loader):\n",
    "        inp, lbl = inp.to(device=args.device, non_blocking=True),\\\n",
    "                        lbl.to(device=args.device, non_blocking=True)\n",
    "        out = model(inp)\n",
    "        loss = criterion(out, lbl)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        if loss >= args.relax_alpha:\n",
    "            loss.backward()\n",
    "        else:\n",
    "            if ep % 2 == 0:\n",
    "                # just negate the loss to do grad ascent\n",
    "                relax_loss = -loss\n",
    "            else:\n",
    "                # flatten probs, compute loss wrt soft labels\n",
    "                with torch.no_grad():\n",
    "                    prob_gt = F.softmax(out, dim=1)[torch.arange(lbl.size(0)), lbl]\n",
    "                    prob_ngt = (1.0 - prob_gt) / (args.num_classes - 1)\n",
    "                    onehot = F.one_hot(lbl, num_classes=10)\n",
    "                    soft_labels = onehot * prob_gt.unsqueeze(-1).repeat(1, args.num_classes) \\\n",
    "                                       + (1 - onehot) * prob_ngt.unsqueeze(-1).repeat(1, args.num_classes)\n",
    "                relax_loss = criterion(out, soft_labels)    \n",
    "            relax_loss.backward()\n",
    "                \n",
    "        opt.step()\n",
    "        # keeping track of tr loss (wrt hard labels)\n",
    "        tr_loss += loss.item()*out.shape[0]\n",
    "        _, pred_lbl = torch.max(out, 1)\n",
    "        tr_acc += torch.sum(torch.eq(pred_lbl.view(-1), lbl)).item()\n",
    "      \n",
    "    # inference on val data after epoch\n",
    "    with torch.inference_mode():\n",
    "        tr_loss, tr_acc = tr_loss/len(tr_loader.dataset), tr_acc/len(tr_loader.dataset)\n",
    "        v_loss, v_acc = ut.get_loss_n_accuracy(model, v_loader, device=args.device)\n",
    "        #loggers\n",
    "        writer.add_scalar('Train/Loss', tr_loss, ep)\n",
    "        writer.add_scalar('Train/Acc', tr_acc, ep)\n",
    "        writer.add_scalar('Val/Loss', v_loss, ep)\n",
    "        writer.add_scalar('Val/Acc', v_acc, ep)\n",
    "        print(f'|Tr/Val Loss: {tr_loss:.3f} / {v_loss:.3f}|', end='--')\n",
    "        print(f'|Tr/Val Acc: {tr_acc:.3f} / {v_acc:.3f}|', end='\\r')\n",
    "        #lr scheduler\n",
    "        scheduler.step()\n",
    "        # early stopping\n",
    "        if args.early_stop:\n",
    "            #scheduler.step(v_loss)\n",
    "            es(v_loss)\n",
    "            if es.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference of test data\n",
    "te_loss, te_acc = ut.get_loss_n_accuracy(model, te_loader, device=args.device)\n",
    "writer.add_scalar('Test/Loss', te_loss, 0)\n",
    "writer.add_scalar('Test/Acc', te_acc, 0)\n",
    "print(f'|Te Loss/Acc: {te_loss:.3f} / {te_acc:.3f}|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2594df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss vals for all data pts in tr/test\n",
    "tr_losses = ut.get_loss_vals(model, tr_loader, device=args.device)\n",
    "te_losses = ut.get_loss_vals(model, te_loader, device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ea88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp. var/mean of losses\n",
    "tr_loss_var, tr_loss_mean = torch.var_mean(tr_losses, unbiased=False)\n",
    "te_loss_var, te_loss_mean = torch.var_mean(te_losses, unbiased=False)\n",
    "writer.add_scalar('Train/Avg.Loss', tr_loss_mean, 0)\n",
    "writer.add_scalar('Train/Sample Var.', tr_loss_var, 0)\n",
    "writer.add_scalar('Test/Avg.Loss', te_loss_mean, 0)\n",
    "writer.add_scalar('Test/Sample Var.', te_loss_var, 0)\n",
    "print(f'|Tr Avg.Loss/Sample Var.: {tr_loss_mean:.3f} / {tr_loss_var:.3f}|')\n",
    "print(f'|Te Avg.Loss/Sample Var.: {te_loss_mean:.3f} / {te_loss_var:.3f}|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack performance (bal.acc, true-positive-rate, false-positive-rate) \n",
    "(ds_bacc, ds_tpr, ds_fpr), _ = at.mia_by_threshold(model, tr_loader, te_loader, threshold=tr_loss_mean)\n",
    "writer.add_scalar('Attack/Bacc', ds_bacc, 0)\n",
    "writer.add_scalar('Attack/TPR', ds_tpr, 0)\n",
    "writer.add_scalar('Attack/FPR', ds_fpr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://stackoverflow.com/questions/47999159/normalizing-two-histograms-in-the-same-plot\n",
    "##computing the bin properties (same for both distributions)\n",
    "num_bin = 50\n",
    "bin_lims = np.linspace(0,5,num_bin+1)\n",
    "bin_centers = 0.5*(bin_lims[:-1]+bin_lims[1:])\n",
    "bin_widths = bin_lims[1:]-bin_lims[:-1]\n",
    "\n",
    "##computing the histograms\n",
    "hist1, _ = np.histogram(tr_losses.cpu().detach().numpy(), bins=bin_lims)\n",
    "hist2, _ = np.histogram(te_losses.cpu().detach().numpy(), bins=bin_lims)\n",
    "\n",
    "##normalizing\n",
    "hist1b = hist1/np.sum(hist1)\n",
    "hist2b = hist2/np.sum(hist2)\n",
    "\n",
    "# plotting\n",
    "plt.bar(bin_centers, hist1b, width = bin_widths, alpha = 0.5, align = 'center', color='tab:green', label='train')\n",
    "plt.bar(bin_centers, hist2b, width = bin_widths, alpha = 0.5, align = 'center',  color='tab:red', label='test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Normalized Frequency')\n",
    "plt.xlabel('Loss')\n",
    "plt.xticks(np.arange(0, 5, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../plots/{file_name}', dpi=150, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0858ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db772567c4567f4b3d1f56ed53e5d7229d15382f6d3dfec57e0c55f7b5cf2dd9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
